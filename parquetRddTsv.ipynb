{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Create a SparkSQL context object\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the Parquet schema file\n",
    "schemaRdd = sc.textFile('/Users/kdunn/Google Drive/TWC/DDL/schema_34.avsc', 1).collect()\n",
    "\n",
    "# Merge the list of lines into a readable string\n",
    "mergedString = ' \\n'.join(schemaRdd)\n",
    "\n",
    "# Convert the string into a Python dictionary, replacing keywords with Python equivalents\n",
    "schemaDict = eval(mergedString.replace('null', 'None').replace('false', 'False').replace('true', 'True'))\n",
    "\n",
    "# Take the name of every top-level field and put in a nice list   \n",
    "topLevelFields = [k['name'] for k in schemaDict[\"fields\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the Parquet data\n",
    "data = sqlCtx.parquetFile(\"/Users/kdunn/Desktop/TWC/schema_30_b6db110d-881c-434a-9746-360ce49c0af7.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import gzip\n",
    "\n",
    "# Create a formattable template string (this also defines the db table columns)\n",
    "templateString = \"%(\" + \")s\\t%(\".join(topLevelFields) + \")s\"\n",
    "\n",
    "# Initialize every top-level field in the schema with a 'Null' string\n",
    "fieldsAndNulls = zip([f.strip('') for f in topLevelFields ], ['Null']*len(topLevelFields))\n",
    "\n",
    "# Initialize the path to write the output to\n",
    "targetFlatFile = \"/Users/kdunn/Desktop/TWC/schemaFilteredFlat.tsv.gz\"\n",
    "\n",
    "def parquetToTSV(data):\n",
    "    # Reference the path as a global since <RDD>.foreach(myFunction) doesn't\n",
    "    # allow passing additional arguments into the function\n",
    "    global targetFlatFile\n",
    "    \n",
    "    # Convert the PySpark RDD object into a Python Dict\n",
    "    tempDict = data.asDict()\n",
    "    \n",
    "    #####\n",
    "    #####\n",
    "    ##### BEGIN: customer data-specific section \n",
    "    #####\n",
    "    #####\n",
    "    \n",
    "    # Pre-filter conditions for Austin area only\n",
    "    if 'customeraccount_divisionid' not in tempDict:\n",
    "        return\n",
    "    else:\n",
    "        division = tempDict['customeraccount_divisionid']\n",
    "        if division is not None and division.upper() != 'STX':\n",
    "            return\n",
    "        else: \n",
    "            if 'customeraccount_city' in tempDict:\n",
    "                city = tempDict['customeraccount_divisionid']\n",
    "                if city is not None and city.lower() != \"austin\":\n",
    "                    return\n",
    "            else:\n",
    "                # No defined city in the record\n",
    "                # means we can't explicity exclude it\n",
    "                goOn = True\n",
    "                \n",
    "   \n",
    "    # Create a dictionary from the key-value pairs\n",
    "    # for building the record dictionary\n",
    "    finalDict = dict( fieldsAndNulls )\n",
    "    \n",
    "    # Update the dictionary with this record's\n",
    "    # data for every column\n",
    "    finalDict.update(tempDict)\n",
    "   \n",
    "    # Flatten the key/val[,val] in the annotations section\n",
    "    annotationsSection = list()\n",
    "    if tempDict['annotations'] is not None:\n",
    "        for key, val in tempDict['annotations'].items():\n",
    "            if val.asDict() is not None:\n",
    "                for param, op in val.asDict().items():\n",
    "                    #print '='.join([str(param), str(op)])\n",
    "                    try:\n",
    "                        annotationsSection.append(\"{k}:\".format(k=key) + '='.join([param, op]))\n",
    "                    except TypeError:\n",
    "                        annotationsSection.append(\"{k}:\".format(k=key) + '='.join([param, 'Null']))\n",
    "                    \n",
    "                        \n",
    "    annotationsSectionString = 'Null'\n",
    "    if len(annotationsSection) > 0:\n",
    "        annotationsSectionString = \"|^|\".join(annotationsSection)\n",
    "\n",
    "    # Flatten content group fields (including the nested genre array)\n",
    "    contentSection = list()\n",
    "    for (k, v) in tempDict['content'].asDict().iteritems():\n",
    "        theVal = v\n",
    "        # Flatten the nested array\n",
    "        if k == 'contentmetadata_genres':\n",
    "            if v is not None:\n",
    "                if isinstance(v, list):\n",
    "                    theVal = '%^%'.join(v)\n",
    "                else:\n",
    "                    theVal = v\n",
    "            else:\n",
    "                theVal = 'Null'\n",
    "\n",
    "        try:\n",
    "            contentSection.append(\"{k}={v}\".format(k=k, v=theVal.encode('utf-8', 'ignore')))\n",
    "        except AttributeError:\n",
    "            contentSection.append(\"{k}={v}\".format(k=k, v='Null'))\n",
    "        #except UnicodeEncodeError:\n",
    "            #pass\n",
    "            #continue #contentSection.append(\"{k}={v}\".format(k=k.decode('utf-8', 'ignore'), v=theVal))\n",
    "            \n",
    "    contentSectionString = 'Null'\n",
    "    if len(contentSection) > 0:\n",
    "        contentSectionString = \"|^|\".join(contentSection)\n",
    "          \n",
    "    # Flatten the array of upcomingContent group fields (including the nested genre array)\n",
    "    upcomingContentSection = list()\n",
    "    for contentEntry in tempDict['upcomingContent']:\n",
    "        thisContent = list()\n",
    "        for (k, v) in contentEntry.asDict().iteritems():\n",
    "            theVal = v\n",
    "            # Flatten the nested array\n",
    "            if k == 'contentmetadata_genres':\n",
    "                if v is not None:\n",
    "                    if len(v) > 0:\n",
    "                        theVal = '%^%'.join(v)\n",
    "                    else:\n",
    "                        theVal = v\n",
    "                else:\n",
    "                    v = 'Null'             \n",
    "                \n",
    "            try:\n",
    "                thisContent.append(\"{k}={v}\".format(k=k, v=theVal.encode('utf-8', 'ignore')))\n",
    "            except AttributeError:\n",
    "                thisContent.append(\"{k}={v}\".format(k=k, v='Null'))\n",
    "            \n",
    "        if len(thisContent) > 0:\n",
    "            upcomingContentSection.append(\"%^%\".join(thisContent))\n",
    "        else:\n",
    "            upcomingContentSection.append(thisContent)\n",
    "\n",
    "    upcomingContentSectionString = 'Null'\n",
    "    if len(upcomingContentSection) > 0:\n",
    "        upcomingContentSectionString = \"|^|\".join(upcomingContentSection)\n",
    "       \n",
    "    timeshiftformatsString = 'Null'\n",
    "    if 'contentrestriction_timeshiftformats' in tempDict and\\\n",
    "       tempDict['contentrestriction_timeshiftformats'] is not None and\\\n",
    "       len(tempDict['contentrestriction_timeshiftformats']) > 0:\n",
    "        timeshiftformatsString = \"|^|\".join(tempDict['contentrestriction_timeshiftformats'])\n",
    " \n",
    "    demographicsString = 'Null'\n",
    "    if len(tempDict['demographics']) > 0:\n",
    "        demographicsString = \"|^|\".join([\"=\".join([key, val]) for key, val in tempDict['demographics'].items()])\n",
    "  \n",
    "    # Replace the record's nested fields with the flattend versions generated above\n",
    "    finalDict['annotations'] = annotationsSectionString\n",
    "    finalDict['content'] = contentSectionString\n",
    "    finalDict['upcomingContent'] = upcomingContentSectionString\n",
    "    finalDict['contentrestriction_timeshiftformats'] = timeshiftformatsString\n",
    "    finalDict['demographics'] = demographicsString\n",
    "    \n",
    "    # Convert millisecond epoch to UTC string timestamp\n",
    "    finalDict['timestamp_received'] = strftime('%Y-%m-%d %H:%M:%S', gmtime(finalDict['timestamp_received']/1000))\n",
    "    \n",
    "    #####\n",
    "    #####   \n",
    "    ##### END customer data-specific section \n",
    "    #####\n",
    "    #####\n",
    "    \n",
    "    # Make a final pass on the record,\n",
    "    # force our 'Null' key where necessary\n",
    "    # and ensure encoding is correct\n",
    "    for (k,v) in finalDict.iteritems():\n",
    "        if v is None:\n",
    "            finalDict[k] = 'Null'\n",
    "        elif isinstance(v, unicode):\n",
    "            continue\n",
    "        elif isinstance(v, str):\n",
    "            finalDict[k] = v.decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Place the values into consistent columns, tab delimited\n",
    "    theRow = templateString % finalDict\n",
    "    \n",
    "    # gzip compress the output file\n",
    "    with gzip.open(targetFlatFile, 'a+') as theFile:\n",
    "        theFile.write(theRow.encode('utf-8', 'ignore'))\n",
    "        theFile.write(\"\\n\")\n",
    "        theFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "open(targetFlatFile, 'w').close()\n",
    "\n",
    "#%time data.foreach(parquetToTSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 8.85 ms, total: 28.9 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "#%time parquetToTSV(data.take(7)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.49 ms, sys: 801 µs, total: 5.29 ms\n",
      "Wall time: 34 ms\n"
     ]
    }
   ],
   "source": [
    "# This is supposed to parallelize the operation\n",
    "# but seems to no work on local/single node context\n",
    "distData = sc.parallelize(data, 4)\n",
    "%time doIt = distData.foreach(parquetToTSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "CPU times: user 41 µs, sys: 15 µs, total: 56 µs\n",
      "Wall time: 44.1 µs\n"
     ]
    }
   ],
   "source": [
    "%time print(doIt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "\n",
    "# Recorsively glob the parquet files \n",
    "# of interest within a root directory\n",
    "parquetFiles = []\n",
    "for root, dirnames, filenames in os.walk('/Users/kdunn/Desktop/'):\n",
    "  for filename in fnmatch.filter(filenames, '*.parquet'):\n",
    "    parquetFiles.append(os.path.join(root, filename))\n",
    "\n",
    "#print parquetFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.7 ms, sys: 32.9 ms, total: 105 ms\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i, f in enumerate(parquetFiles):\n",
    "    # Load the Parquet data\n",
    "    moreData = sqlCtx.parquetFile(f)\n",
    "    \n",
    "    # Specify the global target path\n",
    "    targetFlatFile = \"/Users/kdunn/Desktop/TWC/schemaFilteredFlat-{}.tsv.gz\".format(i)\n",
    "    \n",
    "    # Make sure it's empty\n",
    "    open(targetFlatFile, 'w').close()\n",
    "    \n",
    "    # Do the thing\n",
    "    moreData.foreach(parquetToTSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
