{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "####\n",
    "####    This script expects a file name parameter containing\n",
    "####    full paths of local Parquet files to \n",
    "####    convert to delimited records according\n",
    "####    to the monolith function below\n",
    "####\n",
    "####    The design assumes several of these jobs to \n",
    "####    be submitted to a standalone Spark cluster\n",
    "####    for parallel processing, each getting a\n",
    "####    different workload file.\n",
    "####\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import path, argv, exit\n",
    "from os import environ, getenv\n",
    "from glob import glob\n",
    "\n",
    "# Used to take epoch Unix timestamp\n",
    "# and make a filename string from it\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# Used for compressing TSVs on write\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "####    BEGIN parameter section\n",
    "####\n",
    "\n",
    "# Spark master\n",
    "# Note: this needs to match what\n",
    "# `hostname` returns on the master\n",
    "sparkMasterHostname = \"Kyle-Dunn-MacBook-Pro.local\"\n",
    "sparkMasterPort = \"7077\"\n",
    "\n",
    "# Resources to use\n",
    "numExecutors = 1\n",
    "coresPerExecutor = 1\n",
    "memoryPerExecutor = \"1500m\"\n",
    "\n",
    "# File to look in for list of Parquet files\n",
    "# to be converted\n",
    "# Note: these should be the *full* path\n",
    "inputFilename = argv[1]\n",
    "\n",
    "# Location of the Parquet/Avro schema file\n",
    "schemaFile = '/Users/kdunn/Desktop/TWC/schema_34.avsc'\n",
    "\n",
    "# Directory to place output files under\n",
    "outputFilesRootDir = '/Users/kdunn/Desktop/TWC/'\n",
    "\n",
    "# Output column delimiter\n",
    "columnDelimiter = \"\\t\"\n",
    "\n",
    "# Null string\n",
    "nullMarker = 'Null'\n",
    "\n",
    "# Outer-most level delimiter\n",
    "outerDelim = \"|^|\"\n",
    "\n",
    "# Nested level 1 delimiter\n",
    "levelOneDelim = \"=\"\n",
    "\n",
    "# Nested level 2 delimiter\n",
    "levelTwoDelim = \"%^%\"\n",
    "\n",
    "####\n",
    "####    END parameter section\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPARK_HOME = getenv('SPARK_HOME', None)\n",
    "if SPARK_HOME:\n",
    "    environ['PATH'] = \"%s:%s/bin\" % (getenv(\"PATH\", \"\"), SPARK_HOME)\n",
    "else:\n",
    "    print( \"SPARK_HOME needs to be set (eg. export SPARK_HOME=/opt/spark)\" )\n",
    "    exit(3)\n",
    "    \n",
    "path.insert(0, '/'.join([SPARK_HOME, 'python']))\n",
    "path.append('/'.join([SPARK_HOME, 'python/lib/py4j-*-src.zip']))\n",
    "\n",
    "# Can only import these if SPARK_HOME and PYTHONPATH are set\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set Spark context parameters\n",
    "config = SparkConf().setMaster('spark://{host}:{port}'.format(host=sparkMasterHostname, port=sparkMasterPort))\n",
    "config.set(\"spark.num.executors\", numExecutors)\n",
    "config.set(\"spark.cores.max\", coresPerExecutor)\n",
    "config.set(\"spark.executor.memory\", memoryPerExecutor)\n",
    "\n",
    "config.setAppName(\"Parquet file subset {s}\".format(s=inputFilename))\n",
    "\n",
    "# Get a Spark context from the configuration\n",
    "sc = SparkContext(conf=config)\n",
    "\n",
    "# Create a SparkSQL context object from the SparkContext object\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Parquet schema file\n",
    "schemaRdd = sc.textFile(schemaFile, 1).collect()\n",
    "\n",
    "# Merge the list of lines into a readable string\n",
    "mergedString = ' \\n'.join(schemaRdd)\n",
    "\n",
    "# Convert the string into a Python dictionary, replacing keywords with Python equivalents\n",
    "schemaDict = eval(mergedString.replace('null', 'None').replace('false', 'False').replace('true', 'True'))\n",
    "\n",
    "# Take the name of every top-level field and put in a nice list\n",
    "topLevelFields = [k['name'] for k in schemaDict[\"fields\"]]\n",
    "\n",
    "# Create a formattable template string (this also defines the db table columns)\n",
    "templateString = \"%(\" + \")s\" + columnDelimiter + \"%(\".join(topLevelFields) + \")s\"\n",
    "\n",
    "# Initialize every top-level field in the schema with a 'Null' string\n",
    "fieldsAndNulls = zip([f.strip('') for f in topLevelFields],[nullMarker]*len(topLevelFields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Automagically turn nested Parquet into TSV with *very* wide text columns\n",
    "def parquetToTSV(data):\n",
    "    # Reference the path as a global since <RDD>.foreach(myFunction) doesn't\n",
    "    # allow passing additional arguments into the function\n",
    "    outputFilesRootDir = \"/mapr/user/pivotal\"\n",
    "\n",
    "    # Convert the PySpark RDD object into a Python Dict\n",
    "    tempDict = data.asDict()\n",
    "\n",
    "    #####\n",
    "    #####\n",
    "    ##### BEGIN: customer data-specific section\n",
    "    #####\n",
    "    #####\n",
    "\n",
    "    # Pre-filter conditions for Austin area only\n",
    "    if 'customeraccount_divisionid' not in tempDict:\n",
    "        return\n",
    "    else:\n",
    "        division = tempDict['customeraccount_divisionid']\n",
    "        if division is not None and division.upper() != 'STX':\n",
    "            return\n",
    "        else:\n",
    "            if 'customeraccount_city' in tempDict:\n",
    "                city = tempDict['customeraccount_divisionid']\n",
    "                if city is not None and city.lower() != \"austin\":\n",
    "                    return\n",
    "            else:\n",
    "                # No defined city in the record\n",
    "                # means we can't explicity exclude it\n",
    "                goOn = True\n",
    "\n",
    "\n",
    "    # Create a dictionary from the key-value pairs\n",
    "    # for building the record dictionary\n",
    "    finalDict = dict( fieldsAndNulls )\n",
    "\n",
    "    # Update the dictionary with this record's\n",
    "    # data for every column\n",
    "    finalDict.update(tempDict)\n",
    "\n",
    "    # Flatten the key/val[,val] in the annotations section\n",
    "    annotationsSection = list()\n",
    "    if tempDict['annotations'] is not None:\n",
    "        for key, val in tempDict['annotations'].items():\n",
    "            if val.asDict() is not None:\n",
    "                for param, op in val.asDict().items():\n",
    "                    #print '='.join([str(param), str(op)])\n",
    "                    try:\n",
    "                        annotationsSection.append(\"{k}:\".format(k=key) + levelOneDelim.join([param, op]))\n",
    "                    except TypeError:\n",
    "                        annotationsSection.append(\"{k}:\".format(k=key) + levelOneDelim.join([param, nullMarker]))\n",
    "\n",
    "\n",
    "    annotationsSectionString = nullMarker\n",
    "    if len(annotationsSection) > 0:\n",
    "        annotationsSectionString = outerDelim.join(annotationsSection)\n",
    "\n",
    "    # Flatten content group fields (including the nested genre array)\n",
    "    contentSection = list()\n",
    "    for (k, v) in tempDict['content'].asDict().iteritems():\n",
    "        theVal = v\n",
    "        # Flatten the nested array\n",
    "        if k == 'contentmetadata_genres':\n",
    "            if v is not None:\n",
    "                if isinstance(v, list):\n",
    "                    theVal = levelTwoDelim.join(v)\n",
    "                else:\n",
    "                    theVal = v\n",
    "            else:\n",
    "                theVal = nullMarker\n",
    "\n",
    "        try:\n",
    "            contentSection.append(\"{k}{d}{v}\".format(k=k, d=levelOneDelim, v=theVal.encode('utf-8', 'ignore')))\n",
    "        except AttributeError:\n",
    "            contentSection.append(\"{k}{d}{v}\".format(k=k, d=levelOneDelim, v=nullMarker))\n",
    "        #except UnicodeEncodeError:\n",
    "            #pass\n",
    "            #continue #contentSection.append(\"{k}={v}\".format(k=k.decode('utf-8', 'ignore'), v=theVal))\n",
    "\n",
    "    contentSectionString = nullMarker\n",
    "    if len(contentSection) > 0:\n",
    "        contentSectionString = outerDelim.join(contentSection)\n",
    "\n",
    "    # Flatten the array of upcomingContent group fields (including the nested genre array)\n",
    "    upcomingContentSection = list()\n",
    "    for contentEntry in tempDict['upcomingContent']:\n",
    "        thisContent = list()\n",
    "        for (k, v) in contentEntry.asDict().iteritems():\n",
    "            theVal = v\n",
    "            # Flatten the nested array\n",
    "            if k == 'contentmetadata_genres':\n",
    "                if v is not None:\n",
    "                    if len(v) > 0:\n",
    "                        theVal = levelTwoDelim.join(v)\n",
    "                    else:\n",
    "                        theVal = v\n",
    "                else:\n",
    "                    v = nullMarker\n",
    "\n",
    "            try:\n",
    "                thisContent.append(\"{k}{d}{v}\".format(k=k, d=levelOneDelim, v=theVal.encode('utf-8', 'ignore')))\n",
    "            except AttributeError:\n",
    "                thisContent.append(\"{k}{d}{v}\".format(k=k, d=levelOneDelim, v=nullMarker))\n",
    "\n",
    "        if len(thisContent) > 0:\n",
    "            upcomingContentSection.append(levelTwoDelim.join(thisContent))\n",
    "        else:\n",
    "            upcomingContentSection.append(thisContent)\n",
    "\n",
    "    upcomingContentSectionString = nullMarker\n",
    "    if len(upcomingContentSection) > 0:\n",
    "        upcomingContentSectionString = outerDelim.join(upcomingContentSection)\n",
    "\n",
    "    timeshiftformatsString = nullMarker\n",
    "    if 'contentrestriction_timeshiftformats' in tempDict and\\\n",
    "       tempDict['contentrestriction_timeshiftformats'] is not None and\\\n",
    "       len(tempDict['contentrestriction_timeshiftformats']) > 0:\n",
    "        timeshiftformatsString = outerDelim.join(tempDict['contentrestriction_timeshiftformats'])\n",
    "\n",
    "    demographicsString = nullMarker\n",
    "    if len(tempDict['demographics']) > 0:\n",
    "        demographicsString = outerDelim.join([levelOneDelim.join([key, val]) for key, val in tempDict['demographics'].items()])\n",
    "\n",
    "    # Replace the record's nested fields with the flattend versions generated above\n",
    "    finalDict['annotations'] = annotationsSectionString\n",
    "    finalDict['content'] = contentSectionString\n",
    "    finalDict['upcomingContent'] = upcomingContentSectionString\n",
    "    finalDict['contentrestriction_timeshiftformats'] = timeshiftformatsString\n",
    "    finalDict['demographics'] = demographicsString\n",
    "\n",
    "    # Make a YEAR-MONTH-DAY-HOUR string from the timestamp to be used for a filename\n",
    "    hourString = strftime('%Y-%m-%d-%H', gmtime(finalDict['timestamp_received']/1000))\n",
    "\n",
    "    # Convert millisecond epoch to UTC string timestamp\n",
    "    finalDict['timestamp_received'] = strftime('%Y-%m-%d %H:%M:%S', gmtime(finalDict['timestamp_received']/1000))\n",
    "\n",
    "    # Initialize the path to write the output to\n",
    "    targetFlatFile = outputFilesRootDir + \"/{time}.tsv.gz\"\n",
    "    targetFlatFile = targetFlatFile.format(time=hourString)\n",
    "\n",
    "    #####\n",
    "    #####\n",
    "    ##### END customer data-specific section\n",
    "    #####\n",
    "    #####\n",
    "\n",
    "    # Make a final pass on the record,\n",
    "    # force the 'Null' key where necessary\n",
    "    # and ensure encoding is correct\n",
    "    for (k, v) in finalDict.iteritems():\n",
    "        if v is None:\n",
    "            finalDict[k] = nullMarker\n",
    "        elif isinstance(v, unicode):\n",
    "            continue\n",
    "        elif isinstance(v, str):\n",
    "            finalDict[k] = v.decode('utf-8', 'ignore')\n",
    "\n",
    "    # Place the values into consistent columns, tab delimited\n",
    "    theRow = templateString % finalDict\n",
    "\n",
    "    # gzip compress the output file\n",
    "    theFile = gzip.GzipFile(targetFlatFile, 'a+')\n",
    "    theFile.write(theRow.encode('utf-8', 'ignore'))\n",
    "    theFile.write(\"\\n\")\n",
    "    theFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 ms, sys: 10.7 ms, total: 37 ms\n",
      "Wall time: 720 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numFiles = len(goodFiles)\n",
    "for i, f in enumerate(goodFiles):\n",
    "    # Load the Parquet data\n",
    "    moreData = sqlCtx.parquetFile(f)\n",
    "\n",
    "    # Do the thing\n",
    "    moreData.foreach(parquetToTSV)\n",
    "    \n",
    "    print \"File\", i, \"\", numFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Parquet files of interest\n",
    "# from a pre-computed list of full\n",
    "# file paths\n",
    "parquetFiles = []\n",
    "with open(inputFilename) as theFile:\n",
    "    parquetFiles = [l.strip() for l in theFile.readlines()]\n",
    "    theFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "from os import walk\n",
    "\n",
    "# Recursively glob the Parquet files \n",
    "# of interest within a root directory\n",
    "parquetFiles = []\n",
    "for root, dirnames, filenames in walk('/Users/kdunn/Desktop/TWC'):\n",
    "  for filename in fnmatch.filter(filenames, '*.parquet'):\n",
    "    parquetFiles.append(path.join(root, filename))\n",
    "\n",
    "#print parquetFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All files in a single directory (preferred)\n",
    "\n",
    "# Loads every file within path into a DataFrame object\n",
    "pFiles = sqlCtx.load(source=\"parquet\",\n",
    "                     path=\"/Users/kdunn/Desktop/TWC/data/\")\n",
    "\n",
    "inParallel = sc.parallelize(pFiles)\n",
    "\n",
    "%time inParallel.foreach(parquetToTSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Files spread across many directories\n",
    "\n",
    "# Create a Python dictionary representation of the Schema\n",
    "# to be used in creation of the DataFrame object containing\n",
    "# schema, initialized with Nulls (below)\n",
    "nullSchemaRdd = dict( fieldsAndNulls )\n",
    "\n",
    "\"\"\"\n",
    "http://mail-archives.apache.org/mod_mbox/spark-user/201503.mbox/%3C14be2870b20.27c8.af1f4bf02661e0e7caf994ede2c15203@prismalytics.io%3E\n",
    "\"\"\";\n",
    "\n",
    "# Initialize a Null RDD containing the master schema to ensure unions succeed\n",
    "parquetCollection = sqlContext.createDataFrame(nullSchemaRdd, schemaDict)\n",
    "#sqlCtx.load(source=\"parquet\", path='/'.join(goodFiles[0].split('/')[:-1])).rdd\n",
    "\n",
    "for rootDir in goodFiles:\n",
    "    thisFile = sqlCtx.load(source=\"parquet\", path='/'.join(rootDir.split('/')[:-1])).rdd\n",
    "    parquetCollection = parquetCollection.union(thisFile)\n",
    "\n",
    "    \n",
    "%time parquetCollection.foreach(parquetToTSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
